{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "tweets = pd.read_csv(\"tweets.csv\", encoding = \"ISO-8859-1\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4712e2af5d8b61f3d4a2c880c2d6bca0d23ce93f"
   },
   "outputs": [],
   "source": [
    "tweets[\"text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6644f3e4bcf78a53c095aea24a2c4858e2c0b54b"
   },
   "outputs": [],
   "source": [
    "import string \n",
    "from nltk.corpus import stopwords \n",
    "stopwords = stopwords.words(\"english\")\n",
    "\n",
    "def _clean(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = \"\".join(x for x in txt if x not in string.punctuation)\n",
    "    words = txt.split()\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    txt = \" \".join(words)\n",
    "    return txt\n",
    "\n",
    "tweets[\"cleaned\"] = tweets[\"text\"].apply(lambda x : _clean(x))\n",
    "tweets[[\"text\", \"cleaned\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4bcedfbed59e2022371d590e64f9274cef2f134a"
   },
   "outputs": [],
   "source": [
    "## Keyword Analysis \n",
    "from collections import Counter\n",
    "complete_text = \" \".join(tweets[\"text\"])\n",
    "clean_text = _clean(complete_text)\n",
    "Counter(clean_text.split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0f031e5ca79fa9609897ac6b3a8b6f86f553849"
   },
   "outputs": [],
   "source": [
    "## Top Mentions \n",
    "mentions = [w for w in complete_text.split() if w.startswith(\"@\")]\n",
    "Counter(mentions).most_common(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7398b0b545862719d8e0df7f3ebdafb4574b3df"
   },
   "outputs": [],
   "source": [
    "## Top HashTags\n",
    "htags = [w for w in complete_text.split() if w.startswith(\"#\")]\n",
    "htags = [w for w in htags if \"demo\" not in w.lower()]\n",
    "Counter(htags).most_common(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c124b5bdc653d142482e53c5bcf7ee1e7ff25b4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Top URLs\n",
    "htags = [w for w in complete_text.split() if w.startswith(\"http\")]\n",
    "htags = [w for w in htags if \"demon\" not in w.lower()]\n",
    "Counter(htags).most_common(100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a34b5f09ae492c36e2a71699d9cdef593bc2ea31"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "bigrams = ngrams(clean_text.split(), 2)\n",
    "Counter(bigrams).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "349dc1d721568a5fcedaea608f3387f721e2bb51"
   },
   "outputs": [],
   "source": [
    "## NER \n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag \n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "for text in tweets[\"text\"]:\n",
    "    entities = nltk.ne_chunk(pos_tag(word_tokenize(text))) \n",
    "    for chunk in entities:\n",
    "        if hasattr(chunk, \"label\"):\n",
    "            if \"GPE\" in (str(chunk)):\n",
    "                print (chunk)\n",
    "            if \"ORGANIZATION\" in (str(chunk)):\n",
    "                print (chunk)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5013a987b9501c3810d8692f00155deb30b3591d"
   },
   "outputs": [],
   "source": [
    "## Sentiment analysis \n",
    "from textblob import TextBlob\n",
    "TextBlob(\"many people hate policy changes such as Demonitization\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c0f4c57a87c44a185211fe6601811cef6ef2a90"
   },
   "outputs": [],
   "source": [
    "TextBlob(\"Indians are happy after from Demonitization\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Topic Modelling \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np \n",
    "\n",
    "def generate_topic_models(text):\n",
    "    cvectorizer = CountVectorizer(min_df=4, max_features=2000)\n",
    "    cvz = cvectorizer.fit_transform(text)\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', max_iter=20, random_state=42)\n",
    "    X_topics = lda_model.fit_transform(cvz)\n",
    "\n",
    "    topic_word = lda_model.components_ \n",
    "    vocab = cvectorizer.get_feature_names()\n",
    "    return topic_word, vocab \n",
    "\n",
    "n_top_words = 10\n",
    "topic_word, vocab = generate_topic_models(tweets[\"cleaned\"].values)\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print (\"Topic \" + str(i+1) + \": \" + \" | \".join(topic_words) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1d1439adac3a3eb22c4978b808ba0335d7923b8",
    "collapsed": true
   },
   "source": [
    "## Ideas for information linkings\n",
    "\n",
    "- Descriptive Stats \n",
    "  example : which are the top mentioned persons, which locations are the ones with high negative sentiments etc. \n",
    "- TimeSeries Insights \n",
    "  example : how does the information changes over time \n",
    "- What are the action items\n",
    "- Use this information in Recommendation Engines \n",
    "- Use this information in Machine Learning Models \n",
    "- Use this information to create knowledge banks "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
